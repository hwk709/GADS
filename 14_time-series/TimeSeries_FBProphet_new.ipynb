{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Working With Time Series Data in FBProphet\n",
    "\n",
    "_By Bryce Peake (Washington DC) and Steven Longstreet (Washington DC)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    " \n",
    "**After this lesson, you will be able to:**\n",
    "- Identify time series data.\n",
    "- Explain the challenges of working with time series data.\n",
    "- Use the `datetime` library to represent dates as objects.\n",
    "- Preprocess time series data with Pandas\n",
    "- Create and visualize a Time Series model using FBProphet\n",
    "- Evaluate a Time Series model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"A\">What is a Time Series?</a></h2>\n",
    "\n",
    "A **time series** is a series of data points that's indexed (or listed, or graphed) in time order. Most commonly, a time series is a sequence that's taken at successive equally spaced points in time. Time series are often represented as a set of observations that have a time-bound relation, which is represented as an index.\n",
    "\n",
    "Time series are commonly found in sales, analysis, stock market trends, economic phenomena, and social science problems.\n",
    "\n",
    "These data sets are often investigated to evaluate the long-term trends, forecast the future, or perform some other form of analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look at some Apple stock data to get a feel for what time series data look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:37:54.254957Z",
     "start_time": "2020-03-09T23:37:54.243015Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "%matplotlib inline\n",
    "\n",
    "apple_stock = pd.read_csv(\"./data/aapl.csv\")\n",
    "apple_stock.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"B\">The DateTime library</a></h2>\n",
    "As time is important to time series data, we will need to interpret these data in the ways that humans interpret them (which is many ways). \n",
    "\n",
    "Python's `DateTime` library is great for dealing with time-related data, and Pandas has incorporated this library into its own `datetime` series and objects.\n",
    "\n",
    "In this lesson, we'll review these data types and learn a little more about each of them:\n",
    "\n",
    "* `datetime` objects.\n",
    "* `datetime` series.\n",
    "* Timestamps.\n",
    "* `timedelta()`.\n",
    "\n",
    "### `datetime` Objects\n",
    "Below, we'll load in the `DateTime` library, which we can use to create a `datetime` object by entering in the different components of the date as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:38:31.644807Z",
     "start_time": "2020-03-09T23:38:31.638764Z"
    }
   },
   "outputs": [],
   "source": [
    "apple_stock.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:38:51.779468Z",
     "start_time": "2020-03-09T23:38:51.777434Z"
    }
   },
   "outputs": [],
   "source": [
    "# The datetime library is something you should already have from Anaconda.\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:39:16.024896Z",
     "start_time": "2020-03-09T23:39:16.022672Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's just set a random datetime — not the end of the world or anything.\n",
    "lesson_date = datetime(2012, 12, 21, 12, 21, 12, 844089)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:39:26.454828Z",
     "start_time": "2020-03-09T23:39:26.451676Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(lesson_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:39:40.923538Z",
     "start_time": "2020-03-09T23:39:40.919775Z"
    }
   },
   "outputs": [],
   "source": [
    "#The components of the date are accessible via the object's attributes.\n",
    "print(\"Micro-Second\", lesson_date.microsecond)\n",
    "print(\"Second\", lesson_date.second)\n",
    "print(\"Minute\", lesson_date.minute)\n",
    "print(\"Hour\", lesson_date.hour)\n",
    "print(\"Day\", lesson_date.day)\n",
    "print(\"Month\",lesson_date.month)\n",
    "print(\"Year\", lesson_date.year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `timedelta()`\n",
    "Suppose we want to add time to or subtract time from a date. Maybe we're using time as an index and want to get everything that happened a week before a specific observation.\n",
    "\n",
    "We can use a `timedelta` object to shift a `datetime` object. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:40:35.610790Z",
     "start_time": "2020-03-09T23:40:35.607613Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import timedelta() from the DateTime library.\n",
    "from datetime import timedelta\n",
    "\n",
    "# Timedeltas represent time as an amount rather than as a fixed position.\n",
    "offset = timedelta(days=1, seconds=20)\n",
    "\n",
    "# The timedelta() has attributes that allow us to extract values from it.\n",
    "print('offset days', offset.days)\n",
    "print('offset seconds', offset.seconds)\n",
    "print('offset microseconds', offset.microseconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`datetime`'s `.now()` function will give you the `datetime` object of this very moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:40:50.943830Z",
     "start_time": "2020-03-09T23:40:50.941552Z"
    }
   },
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "print(\"Like Right Now: \", now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current time is particularly useful when using `timedelta()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:40:59.562129Z",
     "start_time": "2020-03-09T23:40:59.559622Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Future: \", now + offset)\n",
    "print(\"Past: \", now - offset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: The largest value a `timedelta()` can hold is days. For instance, you can't say you want your offset to be two years, 44 days, and 12 hours; you have to convert those years to days.*\n",
    "\n",
    "You can read more about the `timedelta()` category [here](https://docs.python.org/2/library/datetime.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T17:47:52.977683Z",
     "start_time": "2019-01-02T17:47:52.973677Z"
    }
   },
   "source": [
    "## Preprocessing Time Series Data with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:41:38.528495Z",
     "start_time": "2020-03-09T23:41:38.509496Z"
    }
   },
   "outputs": [],
   "source": [
    "#Overwrite the original `Date` column with one that's been converted to a `datetime` series.\n",
    "apple_stock['Date'] = pd.to_datetime(apple_stock.Date)\n",
    "apple_stock.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `.dt` Attribute\n",
    "Pandas' `datetime` columns have a `.dt` attribute that allows you to access attributes that are specific to dates. For example:\n",
    "\n",
    "    aapl.Date.dt.day\n",
    "    aapl.Date.dt.month\n",
    "    aapl.Date.dt.year\n",
    "    aapl.Date.dt.weekday_name\n",
    "\n",
    "Check out the Pandas `.dt` [documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dt.html) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:43:39.378895Z",
     "start_time": "2020-03-09T23:43:39.374151Z"
    }
   },
   "outputs": [],
   "source": [
    "apple_stock.Date.dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:42:10.205804Z",
     "start_time": "2020-03-09T23:42:10.201821Z"
    }
   },
   "outputs": [],
   "source": [
    "apple_stock.Date.dt.dayofyear.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timestamps\n",
    "Timestamps are useful objects for comparisons. You can create a timestamp object using the `pd.to_datetime()` function and a string specifying the date. These objects are especially helpful when you need to perform logical filtering with dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:44:23.242472Z",
     "start_time": "2020-03-09T23:44:23.238427Z"
    }
   },
   "outputs": [],
   "source": [
    "ts = pd.to_datetime('1/1/2017')\n",
    "ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:30:02.554950Z",
     "start_time": "2020-03-09T23:30:02.182Z"
    }
   },
   "source": [
    "\n",
    "The main difference between a `datetime` object and a `timestamp` is that timestamps can be used as comparisons.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:44:36.107400Z",
     "start_time": "2020-03-09T23:44:36.098285Z"
    }
   },
   "outputs": [],
   "source": [
    "apple_stock.loc[apple_stock.Date >= ts, :].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by date with Pandas\n",
    "It is easy to filter by date using Pandas. Let's create a subset of data containing only the stock prices from 2017. We can specify the index as a string constant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:45:41.275370Z",
     "start_time": "2020-03-09T23:45:41.272475Z"
    }
   },
   "outputs": [],
   "source": [
    "#first, set the date (as a datetime obj) as the index\n",
    "apple_stock.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:45:42.676940Z",
     "start_time": "2020-03-09T23:45:42.667208Z"
    }
   },
   "outputs": [],
   "source": [
    "#we can ask for only that data from December 2016\n",
    "apple_stock['2016-11']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things to note about indexing with time series. Unlike numeric indexing, the end index will be included. If you want to index with a range, the time indices must be sorted first.  \n",
    "\n",
    "> **Recap:** The steps for preprocessing time series data are to:\n",
    "* Convert time data to a `datetime` object.\n",
    "* Set `datetime` to index the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice\n",
    "Let's use the UFO data set to build a timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:46:02.529628Z",
     "start_time": "2020-03-09T23:46:02.526512Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a `datetime` object representing today's date.\n",
    "datetime.now().second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:46:46.192011Z",
     "start_time": "2020-03-09T23:46:46.142735Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the UFO data set from the internet.\n",
    "\n",
    "#go to this URL https://raw.githubusercontent.com/justmarkham/pandas-videos/master/data/ufo.csv, \n",
    "#use your mouse to copy all of the data\n",
    "\n",
    "ufo = pd.read_clipboard(sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:46:48.311406Z",
     "start_time": "2020-03-09T23:46:48.302967Z"
    }
   },
   "outputs": [],
   "source": [
    "ufo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:49:24.807535Z",
     "start_time": "2020-03-09T23:49:23.547737Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the Time column to a datetime object.\n",
    "ufo[\"Time\"] = pd.to_datetime(ufo.Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:49:24.812257Z",
     "start_time": "2020-03-09T23:49:24.809313Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set the `Time` column to the index of the dataframe.\n",
    "ufo.set_index(\"Time\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:49:25.451280Z",
     "start_time": "2020-03-09T23:49:25.449119Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a `timestamp` object for the date January 1, 1999.\n",
    "timestamp = pd.to_datetime('1/1/1999')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:49:26.015851Z",
     "start_time": "2020-03-09T23:49:26.012690Z"
    }
   },
   "outputs": [],
   "source": [
    "ufo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:49:26.510913Z",
     "start_time": "2020-03-09T23:49:26.506654Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a subset of entries with a date above or equal to January 1, 1999 using a timestamp object.\n",
    "recent_ufos = ufo.loc[ufo.index >= timestamp, :]\n",
    "recent_ufos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating Time Data\n",
    "If we want to investigate trends over time in sales, as always, we'll start by computing simple aggregates. We want to know: What were the mean and median sales in each month and year?\n",
    "\n",
    "We can use `data.resample` on the whole data set and provide:\n",
    "    - A parameter for the level on which to roll up to: `'D'` for day, `'W'` for week, `'M'` for month, `'A'` for year.\n",
    "    - The aggregation method to perform: `mean()`, `median()`, `sum()`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:50:35.700890Z",
     "start_time": "2020-03-09T23:50:35.146552Z"
    }
   },
   "outputs": [],
   "source": [
    "#import the sales data\n",
    "data = pd.read_csv('data/rossmann.csv', skipinitialspace=True, low_memory=False)\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data = data.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:52:18.082048Z",
     "start_time": "2020-03-09T23:52:17.989314Z"
    }
   },
   "outputs": [],
   "source": [
    "data[['Sales']].resample('M').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:53:01.733209Z",
     "start_time": "2020-03-09T23:53:01.640410Z"
    }
   },
   "outputs": [],
   "source": [
    "data[['Sales']].resample('M').std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Statistics\n",
    "With time series, we can \"roll\" statistics across time. For example, the rolling mean is the mean of a moving window across time periods. Pandas offers a variety of functionalities for creating rolling statistics, which we'll only scratch the surface of here.\n",
    "\n",
    "E.g., to understand holidays sales, we don't want to compare sales data in late December with the entire month but instead with a few days immediately surrounding it. We can do this using rolling averages.\n",
    "\n",
    "The syntax for these can be a little tricky at first. We'll be using a `rolling()` function with a statistical function chained to it. Let's dive into more detail.\n",
    "\n",
    "### Parameters for `rolling()` Functions\n",
    "\n",
    "`rolling().mean()` (as well as `rolling().median())` can take the following parameters:\n",
    "\n",
    "* The first indicates the time series to aggregate.\n",
    "* `window` indicates the number of periods to include in the average.\n",
    "* `center` indicates whether the window should be centered on the date or use data prior to that date.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the rolling daily sum over all stores.\n",
    "Use the `.resample()` function to calculate the daily total over all of the stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:54:39.136788Z",
     "start_time": "2020-03-09T23:54:39.047555Z"
    }
   },
   "outputs": [],
   "source": [
    "daily_store_sales = data[['Sales']].resample('D').sum()\n",
    "daily_store_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `.rolling()` function to calculate the rolling average over a three-day period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:55:29.685376Z",
     "start_time": "2020-03-09T23:55:29.675727Z"
    }
   },
   "outputs": [],
   "source": [
    "daily_store_sales.rolling(window=3, center=False).mean().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:55:45.625731Z",
     "start_time": "2020-03-09T23:55:45.617112Z"
    }
   },
   "outputs": [],
   "source": [
    "#We can use our index filtering to just look at 2015\n",
    "daily_store_sales.rolling( window=7, center=True).mean()['2015'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of plotting the full time series, we can plot the rolling mean instead, which smooths random changes in sales and removes outliers, helping us identify larger trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:56:22.699304Z",
     "start_time": "2020-03-09T23:56:22.517202Z"
    }
   },
   "outputs": [],
   "source": [
    "daily_store_sales.rolling(window=30, center=True).mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Expanding Mean\n",
    "The expanding mean simply uses all of the data points up to the current time to calculate the mean, as opposed to a moving window.\n",
    "\n",
    "#### Calculate and plot the expanding mean below. Resample by quarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:57:15.320922Z",
     "start_time": "2020-03-09T23:57:15.181113Z"
    }
   },
   "outputs": [],
   "source": [
    "rolling_mean = data.Sales.resample('Q').sum().rolling(window=1, center=False).mean()\n",
    "expanding_mean = data.Sales.resample('Q').sum().expanding().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:57:16.299487Z",
     "start_time": "2020-03-09T23:57:16.091584Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [16,9]\n",
    "fig, ax = plt.subplots()\n",
    "rolling_mean.plot(legend = True)\n",
    "expanding_mean.plot(legend = True)\n",
    "ax.legend(['Rolling Mean', 'Expanding Mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponentially Weighted Windows\n",
    "Exponentially weighted windows are one of the most common and effective ways of averaging out noise in time series data. The averaging is done with an \"exponential decay\" on the contribution of prior means, decreasing the contribution of time points that are further in the past.\n",
    "\n",
    "The (adjusted) exponentially weighted mean for time, $t$, is defined as:\n",
    "\n",
    "<a id=\"-xt--fracxt-----alphaxt------alphaxt--------alphatx------alpha-----alpha-------alphat-\"></a>\n",
    "### $$ x_t = \\frac{x_t + (1 - \\alpha)x_{t-1} + (1 - \\alpha)^2x_{t-1} + ... + (1 - \\alpha)^{t}x_0} {1 + (1 - \\alpha) + (1 - \\alpha)^2 + ... + (1 - \\alpha)^{t}} $$\n",
    "\n",
    "> **Note:** Review Pandas' [documentation](http://pandas.pydata.org/pandas-docs/stable/computation.html#exponentially-weighted-windows) for more information.\n",
    "\n",
    "**Calculate and plot the exponentially weighted sum along with the rolling sum. What's the difference?**\n",
    "\n",
    "For example: `.resample('Q').sum().ewm(span=10).mean()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:58:21.629681Z",
     "start_time": "2020-03-09T23:58:21.491620Z"
    }
   },
   "outputs": [],
   "source": [
    "rolling_mean = data.Sales.resample('Q').sum().rolling(window=2, center=True).mean()\n",
    "exp_weighted_mean = data.Sales.resample('Q').sum().ewm(span=10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:58:22.764377Z",
     "start_time": "2020-03-09T23:58:22.547848Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [16,9]\n",
    "fig, ax = plt.subplots()\n",
    "rolling_mean.plot(legend = True)\n",
    "exp_weighted_mean.plot(legend = True)\n",
    "ax.legend(['Rolling Mean', 'Exponentially Weighted Mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note that rolling doesn't understand if you are missing periods (e.g., if you roll over three days and your data are missing weekends, it'll roll Fri/Sat/Sun), so you need to resample first if you care about that.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shifting and Lagging Time Series Data\n",
    "Another common operation on time series data is shifting or lagging values backward and forward in time. This can help us calculate the percentage of change from sample to sample. Pandas has a `.shift()` method for shifting the data in a DataFrame.\n",
    "\n",
    "Let's take a look at the Rossman data when we apply lagged features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:01:37.018117Z",
     "start_time": "2020-03-10T00:01:36.968115Z"
    }
   },
   "outputs": [],
   "source": [
    "shifted_forward = data.shift(1)\n",
    "shifted_forward.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Notice that the first row now contains NaN values because there wasn't a previous day's data to shift to that day.*\n",
    "\n",
    "Next, let's shift the sales prices by five days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:30:39.870433Z",
     "start_time": "2020-03-09T23:30:39.827433Z"
    }
   },
   "outputs": [],
   "source": [
    "shifted_forward5 = data.shift(5)\n",
    "shifted_forward5.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use negative numbers to shift the sales values in the reverse direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:30:40.680232Z",
     "start_time": "2020-03-09T23:30:40.637097Z"
    }
   },
   "outputs": [],
   "source": [
    "shifted_backward = data.shift(-1)\n",
    "shifted_backward.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lags can be used to calculate the changes in the values you are tracking with your time series data. In this case, we can use Pandas' `.shift()` method to look at the changes in sales. \n",
    "\n",
    "Let's create a new column in our Rossman DataFrame that contains the previous day's sales. \n",
    "\n",
    "*Note that we add `.copy()` to the end of the chained assignment to explicitly tell Pandas that this will be a copy and not a view. Here is a useful [video](https://www.youtube.com/watch?v=4R4WsDJ-KVc) that helps explain how to avoid SettingCopyWithWarning errors in Pandas.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:02:13.737597Z",
     "start_time": "2020-03-10T00:02:13.713308Z"
    }
   },
   "outputs": [],
   "source": [
    "data['Prev Day Sales'] = data['Sales'].shift(1).copy()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our new column, it's simple to calculate the one-day change in sales at Store 1. Let's create a new column for this value in our DataFrame as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:02:36.783703Z",
     "start_time": "2020-03-10T00:02:36.766321Z"
    }
   },
   "outputs": [],
   "source": [
    "data['Sales Change'] = data['Sales'] - data['Prev Day Sales'].copy()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:02:54.463645Z",
     "start_time": "2020-03-10T00:02:54.449948Z"
    }
   },
   "outputs": [],
   "source": [
    "data[\"Percentage Change\"] = ((data['Sales'] / data[\"Prev Day Sales\"]) * 100).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:02:55.185520Z",
     "start_time": "2020-03-10T00:02:55.170943Z"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series modeling with FBProphet\n",
    "You will need to install prophet for this notebook. While we've used `pip install` in the past, it often causes errors. So, we'll use an alternative: `conda install -c conda-forge fbprophet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install fbprophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:05:35.206526Z",
     "start_time": "2020-03-10T00:05:35.200406Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fbprophet import Prophet\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "%matplotlib inline\n",
    " \n",
    "plt.rcParams['figure.figsize']=(20,10)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the data\n",
    "\n",
    "Read the data in from the retail sales CSV file in the examples folder then set the index to the 'date' column. We are also parsing dates in the data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:15:59.217262Z",
     "start_time": "2020-03-10T00:15:59.173136Z"
    }
   },
   "outputs": [],
   "source": [
    "#Read in the bikeshare data, and change the datetime column to a datetime object. DO NOT SET TO INDEX\n",
    "bikes = pd.read_csv('./data/bikeshare.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:08:06.668301Z",
     "start_time": "2020-03-10T00:08:06.663802Z"
    }
   },
   "outputs": [],
   "source": [
    "#example: sales data\n",
    "sales_df = pd.read_csv('./data/retail_sales.csv', parse_dates = True, low_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:08:09.607398Z",
     "start_time": "2020-03-10T00:08:09.600474Z"
    }
   },
   "outputs": [],
   "source": [
    "sales_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for Prophet\n",
    "\n",
    "For prophet to work, we need to change the names of these columns to 'ds' and 'y', so lets just create a new dataframe and keep our old one handy (you'll see why later). The new dataframe will initially be created with an integer index so we can rename the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:08:30.742079Z",
     "start_time": "2020-03-10T00:08:30.739077Z"
    }
   },
   "outputs": [],
   "source": [
    "sales_df.rename(columns={'date':'ds', 'sales':'y'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:08:31.217678Z",
     "start_time": "2020-03-10T00:08:31.210532Z"
    }
   },
   "outputs": [],
   "source": [
    "sales_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now's a good time to take a look at your data.  Plot the data using pandas' ```plot``` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:08:46.643550Z",
     "start_time": "2020-03-10T00:08:46.455503Z"
    }
   },
   "outputs": [],
   "source": [
    "sales_df.set_index('ds').y.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with time-series data, its good to take a look at the data to determine if trends exist, whether it is stationary, has any outliers and/or any other anamolies. Facebook prophet's example uses the log-transform as a way to remove some of these anomolies but it isn't the absolute 'best' way to do this...but given that its the example and a simple data series, I'll follow their lead for now.  Taking the log of a number is easily reversible to be able to see your original data. \n",
    "\n",
    "To log-transform your data, you can use numpy's log() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:10:11.501249Z",
     "start_time": "2020-03-10T00:10:11.497950Z"
    }
   },
   "outputs": [],
   "source": [
    "sales_df['y'] = np.log(sales_df['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:10:12.111345Z",
     "start_time": "2020-03-10T00:10:12.105277Z"
    }
   },
   "outputs": [],
   "source": [
    "sales_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:10:13.728357Z",
     "start_time": "2020-03-10T00:10:13.545423Z"
    }
   },
   "outputs": [],
   "source": [
    "sales_df.set_index('ds').y.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the above chart, the plot looks the same as the first one but just at a different scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now do the same with the bikeshare data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:16:40.848359Z",
     "start_time": "2020-03-10T00:16:40.845054Z"
    }
   },
   "outputs": [],
   "source": [
    "bikes.rename(columns = {'datetime': 'ds', 'count': 'y'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:19:02.900882Z",
     "start_time": "2020-03-10T00:19:02.896376Z"
    }
   },
   "outputs": [],
   "source": [
    "bikes['ds'] = pd.to_datetime(bikes.ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:19:04.523437Z",
     "start_time": "2020-03-10T00:19:04.192477Z"
    }
   },
   "outputs": [],
   "source": [
    "bikes.set_index('ds').y.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:19:11.505508Z",
     "start_time": "2020-03-10T00:19:11.502503Z"
    }
   },
   "outputs": [],
   "source": [
    "bikes['y2'] = np.log(bikes['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:19:12.622024Z",
     "start_time": "2020-03-10T00:19:12.283313Z"
    }
   },
   "outputs": [],
   "source": [
    "bikes.set_index('ds').y2.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:26:54.638012Z",
     "start_time": "2020-03-10T00:26:54.630251Z"
    }
   },
   "outputs": [],
   "source": [
    "bike_2 = bikes.set_index('ds').resample('D').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:27:39.570909Z",
     "start_time": "2020-03-10T00:27:39.552686Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bike_2.reset_index(inplace = True)\n",
    "bike_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The math behind FBProphet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prophet is a procedure for forecasting time series data based on an __additive__ model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects.\n",
    "\n",
    "y(t) = g(t) + s(t) + h(t) + e(t)\n",
    "\n",
    "g(t)\n",
    "\n",
    "- trend models non-periodic changes (i.e. growth over time)\n",
    "\n",
    "s(t)\n",
    "\n",
    "- seasonality presents periodic changes (i.e. weekly, monthly, yearly)\n",
    "\n",
    "h(t)\n",
    "\n",
    "- ties in effects of holidays (on potentially irregular schedules ≥ 1 day(s))\n",
    "\n",
    "e(t)\n",
    "\n",
    "- covers idiosyncratic changes not accommodated by the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure provides two possible trend models for g(t), “a saturating growth model, and a piecewise linear model.”\n",
    "\n",
    "- Linear Trend with Changepoints: For use when there is a 'maximum capacity', this is the default\n",
    "- Saturating Growth Model: For use when there are no constants, and focus is on capacity for growth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Prophet\n",
    "\n",
    "Now, let's set prophet up to begin modeling our data.\n",
    "\n",
    "Note: Since we are using monthly data, you'll see a message from Prophet saying ```Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.```  This is OK since we are workign with monthly data but you can disable it by using ```weekly_seasonality=True``` in the instantiation of Prophet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:42:58.782565Z",
     "start_time": "2020-03-10T00:42:58.306658Z"
    }
   },
   "outputs": [],
   "source": [
    "model_sales = Prophet()\n",
    "model_sales.fit(sales_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forecasting is fairly useless unless you can look into the future, so we need to add some future dates to our dataframe. For this example, I want to forecast 2 years into the future, so I'll built a future dataframe with 24 periods since we are working with monthly data. Note the ```freq='m'``` inclusion to ensure we are adding 24 months of data.\n",
    "\n",
    "This can be done with the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:42:59.346189Z",
     "start_time": "2020-03-10T00:42:59.336086Z"
    }
   },
   "outputs": [],
   "source": [
    "future_sales = model_sales.make_future_dataframe(periods=24, freq = 'm')\n",
    "future_sales.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To forecast this future data, we need to run it through Prophet's model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:43:02.149808Z",
     "start_time": "2020-03-10T00:43:00.571885Z"
    }
   },
   "outputs": [],
   "source": [
    "forecast_sales = model_sales.predict(future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting forecast dataframe contains quite a bit of data, but we really only care about a few columns.  First, let's look at the full dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:43:02.169543Z",
     "start_time": "2020-03-10T00:43:02.152437Z"
    }
   },
   "outputs": [],
   "source": [
    "forecast_sales.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We really only want to look at yhat, yhat_lower and yhat_upper, so we can do that with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:43:02.359324Z",
     "start_time": "2020-03-10T00:43:02.345983Z"
    }
   },
   "outputs": [],
   "source": [
    "forecast_sales[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Prophet results\n",
    "\n",
    "Prophet has a plotting mechanism called ```plot```.  This plot functionality draws the original data (black dots), the model (blue line) and the error of the forecast (shaded blue area)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:31:53.308292Z",
     "start_time": "2020-03-10T00:31:53.046025Z"
    }
   },
   "outputs": [],
   "source": [
    "model_sales.plot(forecast_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Prophet\n",
    "The shaded blue area is the error of the forecast. But we can only eyeball it. Let's look at the R-squared (amount of variance) and Mean Squared Error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:32:34.258105Z",
     "start_time": "2020-03-10T00:32:34.245462Z"
    }
   },
   "outputs": [],
   "source": [
    "#To do this, we have to get the y-hat and original y's from the data\n",
    "metric_df_sales = forecast_sales.set_index('ds')[['yhat']].join(sales_df.set_index('ds').y).reset_index()\n",
    "metric_df_sales.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:32:45.702438Z",
     "start_time": "2020-03-10T00:32:45.698187Z"
    }
   },
   "outputs": [],
   "source": [
    "# The tail has NaN values, because they're predictions - there was no real Y. Let's drop those for model evaluation.\n",
    "metric_df_sales.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:32:56.424247Z",
     "start_time": "2020-03-10T00:32:56.419774Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Let's take a look at the numbers\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "print(\"R-squared: \", r2_score(metric_df_sales.y, metric_df_sales.yhat))\n",
    "print(\"Mean Squared Error: \", mean_squared_error(metric_df_sales.y, metric_df_sales.yhat))\n",
    "print(\"RMSE: \", np.sqrt(mean_squared_error(metric_df_sales.y, metric_df_sales.yhat)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now try with bikeshare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:34:40.729668Z",
     "start_time": "2020-03-10T00:34:36.279099Z"
    }
   },
   "outputs": [],
   "source": [
    "model_bikes = Prophet()\n",
    "model_bikes.fit(bikes[['ds', 'y']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:35:01.115820Z",
     "start_time": "2020-03-10T00:35:01.108819Z"
    }
   },
   "outputs": [],
   "source": [
    "future_bikes = model_bikes.make_future_dataframe(periods=365, freq = 'd')\n",
    "future_bikes.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:40:20.436146Z",
     "start_time": "2020-03-10T00:40:15.368743Z"
    }
   },
   "outputs": [],
   "source": [
    "forecast_bikes = model_bikes.predict(future_bikes)\n",
    "model.plot(forecast_bikes);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:36:55.235641Z",
     "start_time": "2020-03-10T00:36:55.222727Z"
    }
   },
   "outputs": [],
   "source": [
    "metric_df_bikes = forecast_bikes.set_index('ds')[['yhat']].join(bikes.set_index('ds').y).reset_index()\n",
    "metric_df_bikes.dropna(inplace = True)\n",
    "\n",
    "print(\"R-squared: \", r2_score(metric_df_bikes.y, metric_df_bikes.yhat))\n",
    "print(\"Mean Squared Error: \", mean_squared_error(metric_df_bikes.y, metric_df_bikes.yhat))\n",
    "print(\"RMSE: \", np.sqrt(mean_squared_error(metric_df_bikes.y, metric_df_bikes.yhat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:40:58.049012Z",
     "start_time": "2020-03-10T00:40:58.041436Z"
    }
   },
   "outputs": [],
   "source": [
    "bikes.y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Accounting for Seasonality and Trends\n",
    "\n",
    "We can see from this data that there is a spike in the same month each year. While spike could be due to many different reasons, let's assume its because there's a major promotion that this company runs every year at that time, which is in December for this dataset.\n",
    "\n",
    "When patterns repeat over *known, fixed periods* of time within a data set, we call this **seasonality**. A seasonal pattern exists when a series is influenced by factors related to the cyclic nature of time — i.e., time of month, quarter, year, etc. Seasonality is of a fixed and known period, otherwise it is not truly seasonality. Additionally, it must be either attributed to another factor or counted as a set of anomalous events in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T23:31:50.002552Z",
     "start_time": "2020-03-09T23:31:49.997462Z"
    }
   },
   "source": [
    "### Prophet calls them \"holidays\"\n",
    "\n",
    "Because we know this promotion occurs every december, we want to use this knowledge to help prophet better forecast those months, so we'll use prohpet's ```holiday``` construct (explained here https://facebookincubator.github.io/prophet/docs/holiday_effects.html).\n",
    "\n",
    "The holiday object is a pandas dataframe with the holiday and date of the holiday. For this example, the construct would look like this:\n",
    "\n",
    "```promotions = pd.DataFrame({\n",
    "  'holiday': 'december_promotion',\n",
    "  'ds': pd.to_datetime(['2009-12-01', '2010-12-01', '2011-12-01', '2012-12-01',\n",
    "                        '2013-12-01', '2014-12-01', '2015-12-01']),\n",
    "  'lower_window': 0,\n",
    "  'upper_window': 0,\n",
    "})```\n",
    "\n",
    "This ```promotions``` dataframe consisists of promotion dates for Dec in 2009 through 2015,  The ```lower_window``` and ```upper_window``` values are set to zero to indicate that we don't want prophet to consider any other months than the ones listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:42:31.143528Z",
     "start_time": "2020-03-10T00:42:31.139583Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build the promotions dataframe from above here - be sure you understand the syntax and logic!\n",
    "promotions = pd.DataFrame({\n",
    "  'holiday': 'december_promotion',\n",
    "  'ds': pd.to_datetime(['2009-12-01', '2010-12-01', '2011-12-01', '2012-12-01',\n",
    "                        '2013-12-01', '2014-12-01', '2015-12-01']),\n",
    "  'lower_window': 0,\n",
    "  'upper_window': 0,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:42:38.762669Z",
     "start_time": "2020-03-10T00:42:38.754753Z"
    }
   },
   "outputs": [],
   "source": [
    "promotions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember, we need to log-transform for prophet\n",
    "sales_df['y'] = np.log(sales_df['y'])\n",
    "sales_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:43:14.645875Z",
     "start_time": "2020-03-10T00:43:14.228215Z"
    }
   },
   "outputs": [],
   "source": [
    "#Now let's set up prophet to model our data using holidays\n",
    "model_sales = Prophet(holidays=promotions)\n",
    "model_sales.fit(sales_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:43:28.235421Z",
     "start_time": "2020-03-10T00:43:26.683606Z"
    }
   },
   "outputs": [],
   "source": [
    "#We've instantiated the model, so now we need to build our future dates to forecast into!\n",
    "future_sales = model_sales.make_future_dataframe(periods=24, freq = 'm')\n",
    "future_sales.tail()\n",
    "\n",
    "#... and then run our future data through prophet's model\n",
    "forecast_sales = model_sales.predict(future_sales)\n",
    "\n",
    "forecast_sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:43:31.618466Z",
     "start_time": "2020-03-10T00:43:31.610208Z"
    }
   },
   "outputs": [],
   "source": [
    "#while our new df contains a bit of data, we only care about a few features...\n",
    "forecast_sales[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing with holidays!\n",
    "Same as above at first blush!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:43:51.215246Z",
     "start_time": "2020-03-10T00:43:51.062383Z"
    }
   },
   "outputs": [],
   "source": [
    "#use Prophet's .plot() method to visualize your timeseries.\n",
    "model_sales.plot(forecast_sales);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prophet also allows you to examine the ```components``` of a timeseries using the ```.plot_components()``` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:43:56.255434Z",
     "start_time": "2020-03-10T00:43:55.844448Z"
    }
   },
   "outputs": [],
   "source": [
    "model_sales.plot_components(forecast_sales);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why holidays matter\n",
    "Let's re-run our prophet model without holidays, for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:44:30.537262Z",
     "start_time": "2020-03-10T00:44:30.080533Z"
    }
   },
   "outputs": [],
   "source": [
    "model_no_holiday = Prophet()\n",
    "model_no_holiday.fit(sales_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:44:34.700701Z",
     "start_time": "2020-03-10T00:44:34.692162Z"
    }
   },
   "outputs": [],
   "source": [
    "future_no_holiday = model_no_holiday.make_future_dataframe(periods=24, freq = 'm')\n",
    "future_no_holiday.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:44:38.220951Z",
     "start_time": "2020-03-10T00:44:36.750031Z"
    }
   },
   "outputs": [],
   "source": [
    "forecast_no_holiday = model_no_holiday.predict(future_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There probably won't be a massive difference, given the small amount of data with which we're working on this example. But with greater data comes greater variance..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:44:40.760124Z",
     "start_time": "2020-03-10T00:44:40.755064Z"
    }
   },
   "outputs": [],
   "source": [
    "#join the dataframes\n",
    "forecast_sales.set_index('ds', inplace=True)\n",
    "forecast_no_holiday.set_index('ds', inplace=True)\n",
    "compared_df = forecast_sales.join(forecast_no_holiday, rsuffix=\"_no_holiday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:44:43.586955Z",
     "start_time": "2020-03-10T00:44:43.582766Z"
    }
   },
   "outputs": [],
   "source": [
    "#we're only interested in the predictions, and let's move back to the original scale\n",
    "compared_df= np.exp(compared_df[['yhat', 'yhat_no_holiday']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:44:44.993110Z",
     "start_time": "2020-03-10T00:44:44.987076Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a feature that is the percentage difference between holiday vs. none\n",
    "compared_df['diff_per'] = 100 * (compared_df['yhat'] - compared_df['yhat_no_holiday']) / compared_df['yhat_no_holiday']\n",
    "print(\"difference: \", round(compared_df.diff_per.mean(), 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the difference here is less than 10%, that can be a large amount of money left on the table if your business is a global enterprise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To do this, we have to get the y-hat and original y's from the data\n",
    "metric_df_sales_no_holiday = forecast_no_holiday[['yhat']].join(sales_df.set_index('ds').y).reset_index()\n",
    "metric_df_sales_no_holiday.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tail has NaN values, because they're predictions - there was no real Y. Let's drop those for model evaluation.\n",
    "metric_df_sales_no_holiday.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R-squared: \", r2_score(metric_df_sales_no_holiday.y, metric_df_sales_no_holiday.yhat))\n",
    "print(\"Mean Squared Error: \", mean_squared_error(metric_df_sales_no_holiday.y, metric_df_sales_no_holiday.yhat))\n",
    "print(\"RMSE: \", np.sqrt(mean_squared_error(metric_df_sales_no_holiday.y, metric_df_sales_no_holiday.yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The general recipe\n",
    "\n",
    "m = Prophet()\n",
    "\n",
    "m.add_regressor('add1')\n",
    "\n",
    "m.add_regressor('add2')\n",
    "\n",
    "...\n",
    "\n",
    "m.fit(df)\n",
    "\n",
    "future = m.make_future_dataframe(periods=10)\n",
    "\n",
    "forecast = m.predict(future)\n",
    "\n",
    "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note that the additional variables should have values for your future (test) data. If you don't have them, you could start by using FBProphet to predict add1 and add2 with  with univariate timeseries, and then predict y with add_regressor and the predicted add1 and add2 as future values of the additional variables.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:51:04.806932Z",
     "start_time": "2020-03-10T00:51:04.802137Z"
    }
   },
   "outputs": [],
   "source": [
    "temperature = bikes[['ds', 'temp']]\n",
    "temperature.rename(columns = {'temp': 'y'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:54:54.291980Z",
     "start_time": "2020-03-10T00:54:44.089829Z"
    }
   },
   "outputs": [],
   "source": [
    "temp_m = Prophet()\n",
    "temp_m.fit(temperature[['ds', 'y']])\n",
    "temp_m_future = temp_m.make_future_dataframe(periods = 365, freq = 'D')\n",
    "temp_m_forecast = temp_m.predict(future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:54:54.297441Z",
     "start_time": "2020-03-10T00:54:54.294061Z"
    }
   },
   "outputs": [],
   "source": [
    "yhat_temp = temp_m_forecast['yhat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:54:56.468581Z",
     "start_time": "2020-03-10T00:54:54.299976Z"
    }
   },
   "outputs": [],
   "source": [
    "m = Prophet()\n",
    "m.add_regressor('temp')\n",
    "m.fit(bikes[['ds', 'y', 'temp', 'humidity']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T00:55:01.217811Z",
     "start_time": "2020-03-10T00:54:56.471373Z"
    }
   },
   "outputs": [],
   "source": [
    "future = m.make_future_dataframe(periods = 365, freq = 'D')\n",
    "future['temp'] = yhat_temp\n",
    "forecast = m.predict(future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T01:01:31.613677Z",
     "start_time": "2020-03-10T00:59:40.538272Z"
    }
   },
   "outputs": [],
   "source": [
    "#Another approach to evaluation\n",
    "from fbprophet.diagnostics import cross_validation, performance_metrics\n",
    "df_cv = cross_validation(m, horizon='30 days')\n",
    "df_p = performance_metrics(df_cv)\n",
    "df_p.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T22:34:55.177038Z",
     "start_time": "2020-03-09T22:34:55.174818Z"
    }
   },
   "outputs": [],
   "source": [
    "#How did your model do? Does adding additional regressors help or inhibit your model? \n",
    "metric_df_bikes = forecast.set_index('ds')[['yhat']].join(bikes.set_index('ds').y).reset_index()\n",
    "metric_df_bikes.dropna(inplace = True)\n",
    "\n",
    "print(\"R-squared: \", r2_score(metric_df_bikes.y, metric_df_bikes.yhat))\n",
    "print(\"Mean Squared Error: \", mean_squared_error(metric_df_bikes.y, metric_df_bikes.yhat))\n",
    "print(\"RMSE: \", np.sqrt(mean_squared_error(metric_df_bikes.y, metric_df_bikes.yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prophet for Market prediction - lab time!\n",
    "Prophet can detect changepoints in timeseries data, and we can often use it to our advantage. Let's grab FRED economic data and see how this goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download 01/2009 - current S&P500 data at https://fred.stlouisfed.org/series/SP500 and import it into pandas\n",
    "market_df = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now prepare your data for prophet. Hint: prophet needs \"ds\" and a log-transformed \"y\" to work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the model, and fit our data\n",
    "model = Prophet()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the future dataframe, forecasting for 1 year from now. THen create a forecast by passing the future into model.predict()\n",
    "future = model.make_future_dataframe(periods = 365)\n",
    "forecast = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now plot it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's hard to see what's going on here, in part because we have such a condensed visual. Let's look at the last 2 years of forecast vs. actual without looking at the future - we are just interested ing etting a visual of theerror between actual vs. forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start by setting the index of the forecast df to the ds, and joining it to the marekt_df\n",
    "two_years = \n",
    "\n",
    "#now set two_years equal to the last 800 data points\n",
    "two_years = two_years[['SP500', 'yhat', 'yhat_upper', 'yhat_lower' ]].dropna().tail(800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now trasnform the predictions back to the original scale of the data\n",
    "two_years['yhat']=np.exp(two_years.yhat)\n",
    "two_years['yhat_upper']=np.exp(two_years.yhat_upper)\n",
    "two_years['yhat_lower']=np.exp(two_years.yhat_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's visualize the relationship between SP500 and yhat (our prediction) using pandas .plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our forecast does great at trending, but doesn't do well at catching the volatility of the market. This would be very good for 'riding trends', but not so good for catching peaks and dips. \n",
    "\n",
    "We can see this in the numbers as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to tlook at the usefulness of this forecast is to plot the upper and lower confidence bands against the actuals. We can do that with a plot that combines yhat_upper and yhat_lower with the rest into a matplotlib subplot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(two_years.SP500)\n",
    "ax1.plot(two_years.yhat)\n",
    "ax1.plot(two_years.yhat_upper, color='black',  linestyle=':', alpha=0.5)\n",
    "ax1.plot(two_years.yhat_lower, color='black',  linestyle=':', alpha=0.5)\n",
    "\n",
    "ax1.set_title('Actual S&P 500 (Orange) vs S&P 500 Forecasted Upper & Lower Confidence (Black)')\n",
    "ax1.set_ylabel('Price')\n",
    "ax1.set_xlabel('Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw above, if you're trying to do shortterm trading then this model is useless. But if you are investing with a timeframe of months to years, this forecast might provide some value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also look at this from the full df. Here I build it manually again!\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(full_df.SP500)\n",
    "ax1.plot(full_df.yhat, color='black', linestyle=':')\n",
    "ax1.fill_between(full_df.index, np.exp(full_df['yhat_upper']), np.exp(full_df['yhat_lower']), alpha=0.5, color='darkgray')\n",
    "ax1.set_title('Actual S&P 500 (Orange) vs S&P 500 Forecasted (Black) with Confidence Bands')\n",
    "ax1.set_ylabel('Price')\n",
    "ax1.set_xlabel('Date')\n",
    "\n",
    "L=ax1.legend() #get the legend\n",
    "L.get_texts()[0].set_text('S&P 500 Actual') #change the legend text for 1st plot\n",
    "L.get_texts()[1].set_text('S&P 5600 Forecasted') #change the legend text for 2nd plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
